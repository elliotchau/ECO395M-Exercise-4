---
title: Exercise 4
output: github_document
---
By Hana Krijestorac, David Garrett, and Elliot Chau

Problem 1
================
## Clustering and PCA of Wine

```{r, include=FALSE}
# read in data
wine <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv")
# load packages
library(tidyverse)
library(ggplot2)
library(ggfortify) #for plotting pc1 and pc2
library(corrplot)
source("http://www.sthda.com/upload/rquery_cormat.r")

# principal component analysis for wine color
X = wine[,c(1:11)] # unsupervised data only
pc1 = prcomp(X, scale=TRUE, center=TRUE)
loadings = pc1$rotation
scores = pc1$x
```

```{r, include=FALSE}
X = wine[,c(1:11)] # unsupervised data only
pc1 = prcomp(X, scale=TRUE, center=TRUE)
loadings = pc1$rotation
scores = pc1$x
```
### Determining the color of the wine

*Principal component analysis*

We first approach this problem utilizing principal component analysis. After scaling and centering the data, we arrive at 11 componenets with the following characteristics. 

```{r, echo=FALSE}
summary(pc1) # statisitics about each component
```

To illustrate this information, we use a biplot to plot each individual wine (identified by a unique number). The vectors indicate the direction of each chemical characteristic.

```{r, echo=FALSE}
biplot(pc1) # each wine's ID with chemical characteristics vectors
```

Since the prior plot is somewhat difficult to interpret, we will remove the vectors and replace wine identification numbers with dots.

```{r, echo=FALSE}
autoplot(pc1) # a closer look with just the data points
```

We can see two distinct groups which may indicate differences as a result of the wine's color.

```{r, echo=FALSE}
qplot(scores[,1], scores[,2], color=wine$color, xlab='Component 1', ylab='Component 2')
```

Indeed, when applying "supervised" information to add color, the plot is separated into two groups based on color.

To further analyze the data, we create a correlation matrix of how chemical characteristics relate to each other. This may allow us to test various relationships in the next part.

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
rquery.cormat(X)
```

```{r, include=FALSE}
X = scale(X, center=TRUE, scale=TRUE)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")
# run k-means with 2 clusters for each color
cluster1 = kmeans(X, 2, nstart=25)
```
*k-means clustering*

We then run k-means clustering. The summary characteristics for the two clusters is as follows:

```{r, echo=FALSE}
cluster1$center[1,]*sigma + mu
cluster1$center[2,]*sigma + mu
```

Based on this information, we are able to see differences between the two clusters. The second cluster has notably higher acidity levels, lower sulfur levels, and less sugar. The following is an assortment of plots separated by wine color.

```{r, echo=FALSE}
qplot(volatile.acidity, alcohol, data=wine, color=factor(cluster1$cluster))
qplot(residual.sugar, pH, data=wine, color=factor(cluster1$cluster))
qplot(fixed.acidity, chlorides, data=wine, color=factor(cluster1$cluster))
qplot(total.sulfur.dioxide, density, data=wine, color=factor(cluster1$cluster))
qplot(alcohol, density, data=wine, color=factor(cluster1$cluster))
```

We then determine the accuracy of k-means clustering. As we can see, it seems to do an excellent job in clustering wines by their color.

```{r, echo=FALSE}
xtabs(~cluster1$cluster + wine$color)
```

Based on the granularity of analysis available with k-means clustering, we believe it is the technique that makes more sense. We are able to compute accuracy with the use of supervised information, and we are also able to break down comparisons by chemical component.

### Wine quality

We begin by taking a look at the summary statistics for the wine quality column.

```{r, echo=FALSE}
summary(wine$quality)
```

As we can see, the category ranges from a low of 3 and a high of 9. This means there are a total of 7 ratings. 

*Principal component analysis*

```{r, include=FALSE}
pc2 = kmeans(scores[,1:11], 7, nstart=25)
```

We start with principal component analysis. The following plot shows a general ability to create 7 somewhat distinct groups.

```{r, echo=FALSE}
qplot(scores[,1], scores[,2], color=factor(pc2$cluster), xlab='Component 1', ylab='Component 2')
```

However, when utilizing the supervised information to check our work, the groups were not grouped by the quality score. Therefore, PCA is not a technique that can complete this task.

```{r, echo=FALSE}
qplot(scores[,1], scores[,2], color=factor(wine$quality), xlab='Component 1', ylab='Component 2')
```

*k-means clustering*

```{r, include=FALSE}
cluster2 = kmeans(X, 7, nstart=25)
```

We then move on to k-means clustering. Clustering by 7 score categories, the following is a variety of plots.

```{r, echo=FALSE}
qplot(volatile.acidity, alcohol, data=wine, color=factor(cluster2$cluster))
qplot(residual.sugar, pH, data=wine, color=factor(cluster2$cluster))
qplot(fixed.acidity, chlorides, data=wine, color=factor(cluster2$cluster))
qplot(total.sulfur.dioxide, density, data=wine, color=factor(cluster2$cluster))
```

The visual evidence suggests that all the overlapping colors is not so useful in clustering by score. Let's take a look at the raw numbers. 

```{r, echo=FALSE}
xtabs(~cluster2$cluster + wine$quality)
```

The table shows that each of the 7 clusters basically has a random distribution of wines. The k-means technique is also not able to determine wine quality. This is more than likely the result of subjective scores assigned to each wine, with each wine snob assigning different value to certain notes, flavors, and aromas. 

Problem 2
================
## Market segmentation

In order to determine what market segments that NutrientH20 should focus on, we began by dropping all of the users that have been marked with spam and then dropping the spam column altogether.

```{r, include=FALSE}
library(skimr)
social_marketing <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/social_marketing.csv")
str(social_marketing)
skim(social_marketing)
summary(social_marketing)
cor(social_marketing[,2:37])
library(GGally)

table(social_marketing$spam, social_marketing$adult)
table(social_marketing$adult)
prop.table(table(social_marketing$spam, social_marketing$adult))

# Dropping all observations marked with spam
SM_spam_free <- social_marketing[!(social_marketing$spam > "0"),]

# Checking to see if all observations with spam were dropped
table(SM_spam_free$spam)

# Dropping the spam variable
SM <- SM_spam_free[-c(36)]

# Number of users and the amount of adult content they posted
table(SM$adult)
prop.table(table(SM$adult))

sum(SM$adult)

# Find row summery of total tweet count for each user
rs <- rowSums(SM[,-1])

# Drop users with who have more than 25% of total tweets related to adult content
rs_2 <- SM[SM$adult/rs<.25,]
SM <- rs_2

library(dplyr)
library(tidyr)
```

Next, we found which users had more than 25% of their tweets relates to adult content so that any potential pornography bots that the original human annotators of the data may not have captured and deleted. 

```{r, echo=FALSE}
# Check
SM %>% 
  group_by(adult, X) %>%
  # Count occurences of adult flagged tweets per user
  summarise(n=n()) %>%
  # Get percentage per user
  mutate(percent = n / sum(n)*100) %>%
  filter(percent>20) %>%
  select(X, adult, percent) %>%
  arrange(desc(adult))
```

The following a correlation plot of how each group relates to another.

```{r, echo=FALSE}
ggcorr(social_marketing[,2:37])
```

```{r, include=FALSE}
#----PCA----#
# Drop users variable X
SM2 <- SM
```

Due to the multiple dimensions amongst tweets from their followers, we used principal component analysis to reduce the dimensionality of the data. The following is the distribution of our cleaned data.

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
# Check distribution for SM2
SM2 %>% 
  gather(Variable, Value) %>% 
  ggplot(aes(x=Value, fill=Variable)) +
  geom_density(alpha=0.3) +
  geom_vline(aes(xintercept=0)) +
  theme_bw()
```

We continue with principal component analysis.

```{r, include=FALSE}
SM_pca <- prcomp(SM2[,-c(1,10)],
                 scale. = TRUE,
                 center = TRUE)

str(SM_pca)
print(SM_pca)
```

```{r, echo=FALSE}
plot(SM_pca, type = 'l') 
```

```{r, include=FALSE}
SM_pcaDF <- data.frame(SM_pca$x)
```

The following is table about each principal component and a matrix of how they relate to each other.

```{r, echo=FALSE}
summary(SM_pca)
plot(SM_pcaDF[,1:5], pch = 16, col=rgb(0,0,0, alpha = .5), cex =.3)
```

```{r, include=FALSE}
# Plot of PCA
library(ggplot2)
library(ggfortify)
```

```{r, echo=FALSE}
ggplot2::autoplot(SM_pca, loadings = TRUE, label.size = 3, loadings.label = TRUE, loadings.colour = 'red', loadings.label.colour = "blue")
```

```{r, include=FALSE}
# Find loadings
loadings = SM_pca$rotation

# these are the 10 tweet groups most negatively associated with Nutrient H20 followers
loadings[,1] %>% sort %>% head(10)

# these are the 10 tweet groups most positively associated with Nutrient H20 followers
loadings[,1] %>% sort %>% tail(10)

#----Hierarchical Clustering the PCA's for market segmentation----#
x = loadings[,1]
y = loadings[,2:8]
# Cluster and graphing
```

We then used hierarchical clustering on these principal components to find correlational clusters amongst their followers tweets in order to segment the market into different groups.

```{r, echo=FALSE}
# Market segementation using the PCA's 1-8
hc = hclust(dist(cbind(x,y)), method = 'ward.D')
plot(hc, axes=F, xlab='Twitter Categories', ylab='Market Segmentation', sub ='', main='Clustering of Principle Components 1-8')
rect.hclust(hc, k=6, border='red')
```

In doing so we found six main marketing groups based on correlated interests. We believe that by focusing their attention on these six groups, NutrientH20 can direct their content in order to relate to these subgroups to further their sales. 

Problem 3
================
## Association rules for grocery purchases

```{r, include=FALSE}
library(tidyverse)
library(arules)
library(arulesViz)

groceries_raw = read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/groceries.txt", header = FALSE)

groceries_raw$ID = seq.int(nrow(groceries_raw))

grocery <- cbind(groceries_raw[,5], stack(lapply(groceries_raw[,1:4], as.character)))[1:2]
colnames(grocery) <- c("ID","items")
grocery <- grocery[order(grocery$ID),]
grocery <- grocery[!(grocery$items==""),]
row.names(grocery) <- 1:nrow(grocery)
grocery$ID = factor(grocery$ID)

#split 
grocerlist = split(x=grocery$items, f=grocery$ID)

grocerlist = lapply(grocerlist, unique)
grocertrans = as(grocerlist, "transactions")
```

We first start by finding a summary of grocery statistics. We see that "whole milk" and "other vegetables" top the list of transactions.

```{r, echo=FALSE}
summary(grocertrans)
```

We computed 45 association rules. We will narrow down the rules into relevant subsets based on confidence, lift, and support later. 

```{r, echo=FALSE}
groceryrules = apriori(grocertrans, 
                     parameter=list(support=.01, confidence=.1, maxlen=5))
inspect(groceryrules)
```

We then created a subset to only include a lift measure of greater than 2, a confidence measure greater than 0.3, and a subset with support greater than 0.035.


The level of confidence was chosen based on interpretability since it was difficult to find any rules for confidence greater than 4. Confidence measures how often items in itemset Y appear in transactions which contain itemset X. Some rules that were identified through this method was associations like curd and whole milk (perhaps for dairy lovers?), and both root vegetables and other vegetables and whole milk. This could be useful to know in a retail context i.e. where and when to give promotions or ideas for product placement for milk given that you already are in the vegetable section. This is also reflected in the plot of association rules where “whole milk” and “other vegetables” are the two largest clusters.

This is the subset for confidence > 0.3.
```{r, echo=FALSE}
inspect(subset(groceryrules, confidence > 0.3))
```

```{r, include=FALSE}
inspect(subset(groceryrules, lift > 10 & confidence > 0.5))
# plot all the rules in (support, confidence) space
# notice that high lift rules tend to have low support
plot(groceryrules)
```

Support measures the frequency of an itemset within transactions. The choice of a low level of support was driven by the fact that there weren’t too many different grocery items within the dataset. Again, a strong association between “whole milk” and “other vegetables,” perhaps since these are generally staple items. 

This is the subset for support > 0.035.
```{r, echo=FALSE}
# can now look at subsets driven by the plot
inspect(subset(groceryrules, support > 0.035))
```

When looking at lift, the rules that were generated were generally intuitive. For example, if a consumer buys root vegetables, he or she is likely to also purchase “other vegetables,” or has a conditional probability of 2.909. Another intuitive rule that was generated was that the conditional probability of purchasing citrus fruit given that you already bought tropical fruit was 3.477. This make sense considering that grocery stores tend to group these items together and offer promotions for similar items. Again, this is reflected in the overall plot of the 45 association rules where different types of fruits are grouped together in the top right corner. Lift might be the most informative measure since it measures the conditional probability of purchasing itemset X given that you already purchased itemset Y and therefore takes into account statistical dependence, whereas confidence and support do not. 

This is the subset for lift > 2.
```{r, echo=FALSE}
## Choose a subset
inspect(subset(groceryrules, lift > 2))
```

We combine these three measures to create a scatterplot and a two-key plot.

```{r, echo=FALSE}
plot(groceryrules, measure = c("support", "lift"), shading = "confidence")
plot(groceryrules, method='two-key plot')
```

We then find the following 10 “best” associations.

```{r, echo=FALSE}
inspect(subset(groceryrules, lift > 10 & confidence > 0.5))
```

```{r, include=FALSE}
inspect(subset(groceryrules, confidence > 0.6))
# graph-based visualization
sub1 = subset(groceryrules, subset=confidence > 0.01 & support > 0.005)
```

```{r, echo=FALSE}
summary(sub1)
```

```{r, include=FALSE}
plot(sub1, method='graph')
```

We visualize these associations through a network graph. The larger the label, the more frequent the transaction. "Whole milk," "other vegetables," "root vegetables," and "soda" seem to tie together most transctions. 

```{r, echo=FALSE}
plot(head(sub1, 100, by='lift'), method='graph')
```

```{r, include=FALSE}
# export a graph
saveAsGraph(sub1, file = "groceryrules.graphml")
```

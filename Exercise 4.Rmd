---
title: Exercise 4
output: github_document
---
By Hana Krijestorac, David Garrett, and Elliot Chau

Problem 1
================
## Clustering and PCA of Wine

```{r, include=FALSE}
# read in data
wine <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv")
# load packages
library(tidyverse)
library(ggplot2)
library(ggfortify) #for plotting pc1 and pc2
library(corrplot)
source("http://www.sthda.com/upload/rquery_cormat.r")

# principal component analysis for wine color
X = wine[,c(1:11)] # unsupervised data only
pc1 = prcomp(X, scale=TRUE, center=TRUE)
loadings = pc1$rotation
scores = pc1$x
```

```{r, include=FALSE}
X = wine[,c(1:11)] # unsupervised data only
pc1 = prcomp(X, scale=TRUE, center=TRUE)
loadings = pc1$rotation
scores = pc1$x
```
### Determining the color of the wine

*Principal component analysis*

We first approach this problem utilizing principal component analysis. After scaling and centering the data, we arrive at 11 componenets with the following characteristics. 

```{r, echo=FALSE}
summary(pc1) # statisitics about each component
```

To illustrate this information, we use a biplot to plot each individual wine (identified by a unique number). The vectors indicate the direction of each chemical characteristic.

```{r, echo=FALSE}
biplot(pc1) # each wine's ID with chemical characteristics vectors
```

Since the prior plot is somewhat difficult to interpret, we will remove the vectors and replace wine identification numbers with dots.

```{r, echo=FALSE}
autoplot(pc1) # a closer look with just the data points
```

We can see two distinct groups which may indicate differences as a result of the wine's color.

```{r, echo=FALSE}
qplot(scores[,1], scores[,2], color=wine$color, xlab='Component 1', ylab='Component 2')
```

Indeed, when applying "supervised" information to add color, the plot is separated into two groups based on color.

To further analyze the data, we create a correlation matrix of how chemical characteristics relate to each other. This may allow us to test various relationships in the next part.

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
rquery.cormat(X)
```

```{r, include=FALSE}
X = scale(X, center=TRUE, scale=TRUE)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")
# run k-means with 2 clusters for each color
cluster1 = kmeans(X, 2, nstart=25)
```
*k-means clustering*

We then run k-means clustering. The summary characteristics for the two clusters is as follows:

```{r, echo=FALSE}
cluster1$center[1,]*sigma + mu
cluster1$center[2,]*sigma + mu
```

Based on this information, we are able to see differences between the two clusters. The second cluster has notably higher acidity levels, lower sulfur levels, and less sugar. The following is an assortment of plots separated by wine color.

```{r, echo=FALSE}
qplot(volatile.acidity, alcohol, data=wine, color=factor(cluster1$cluster))
qplot(residual.sugar, pH, data=wine, color=factor(cluster1$cluster))
qplot(fixed.acidity, chlorides, data=wine, color=factor(cluster1$cluster))
qplot(total.sulfur.dioxide, density, data=wine, color=factor(cluster1$cluster))
qplot(alcohol, density, data=wine, color=factor(cluster1$cluster))
```

We then determine the accuracy of k-means clustering. As we can see, it seems to do an excellent job in clustering wines by their color.

```{r, echo=FALSE}
xtabs(~cluster1$cluster + wine$color)
```

Based on the granularity of analysis available with k-means clustering, we believe it is the technique that makes more sense. We are able to compute accuracy with the use of supervised information, and we are also able to break down comparisons by chemical component.

### Wine quality

We begin by taking a look at the summary statistics for the wine quality column.

```{r, echo=FALSE}
summary(wine$quality)
```

As we can see, the category ranges from a low of 3 and a high of 9. This means there are a total of 7 ratings. 

*Principal component analysis*

```{r, include=FALSE}
pc2 = kmeans(scores[,1:11], 7, nstart=25)
```

We start with principal component analysis. The following plot shows a general ability to create 7 somewhat distinct groups.

```{r, echo=FALSE}
qplot(scores[,1], scores[,2], color=factor(pc2$cluster), xlab='Component 1', ylab='Component 2')
```

However, when utilizing the supervised information to check our work, the groups were not grouped by the quality score. Therefore, PCA is not a technique that can complete this task.

```{r, echo=FALSE}
qplot(scores[,1], scores[,2], color=factor(wine$quality), xlab='Component 1', ylab='Component 2')
```

*k-means clustering*

```{r, include=FALSE}
cluster2 = kmeans(X, 7, nstart=25)
```

We then move on to k-means clustering. Clustering by 7 score categories, the following is a variety of plots.

```{r, echo=FALSE}
qplot(volatile.acidity, alcohol, data=wine, color=factor(cluster2$cluster))
qplot(residual.sugar, pH, data=wine, color=factor(cluster2$cluster))
qplot(fixed.acidity, chlorides, data=wine, color=factor(cluster2$cluster))
qplot(total.sulfur.dioxide, density, data=wine, color=factor(cluster2$cluster))
```

The visual evidence suggests that all the overlapping colors is not so useful in clustering by score. Let's take a look at the raw numbers. 

```{r, echo=FALSE}
xtabs(~cluster2$cluster + wine$quality)
```

The table shows that each of the 7 clusters basically has a random distribution of wines. The k-means technique is also not able to determine wine quality. This is more than likely the result of subjective scores assigned to each wine, with each wine snob assigning different value to certain notes, flavors, and aromas. 

Problem 2
================
## Market segmentation

In order to determine what market segments that NutrientH20 should focus on, we began by dropping all of the users that have been marked with spam and then dropping the spam column altogether.

```{r, include=FALSE}
library(skimr)
social_marketing <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/social_marketing.csv")
str(social_marketing)
skim(social_marketing)
summary(social_marketing)
cor(social_marketing[,2:37])
library(GGally)

table(social_marketing$spam, social_marketing$adult)
table(social_marketing$adult)
prop.table(table(social_marketing$spam, social_marketing$adult))

# Dropping all observations marked with spam
SM_spam_free <- social_marketing[!(social_marketing$spam > "0"),]

# Checking to see if all observations with spam were dropped
table(SM_spam_free$spam)

# Dropping the spam variable
SM <- SM_spam_free[-c(36)]

# Number of users and the amount of adult content they posted
table(SM$adult)
prop.table(table(SM$adult))

sum(SM$adult)

# Find row summery of total tweet count for each user
rs <- rowSums(SM[,-1])

# Drop users with who have more than 25% of total tweets related to adult content
rs_2 <- SM[SM$adult/rs<.25,]
SM <- rs_2

library(dplyr)
library(tidyr)
```

Next, we found which users had more than 25% of their tweets relates to adult content so that any potential pornography bots that the original human annotators of the data may not have captured and deleted. 

```{r, echo=FALSE}
# Check
SM %>% 
  group_by(adult, X) %>%
  # Count occurences of adult flagged tweets per user
  summarise(n=n()) %>%
  # Get percentage per user
  mutate(percent = n / sum(n)*100) %>%
  filter(percent>20) %>%
  select(X, adult, percent) %>%
  arrange(desc(adult))
```

The following a correlation plot of how each group relates to another.

```{r, echo=FALSE}
ggcorr(social_marketing[,2:37])
```

```{r, include=FALSE}
#----PCA----#
# Drop users variable X
SM2 <- SM
```

Due to the multiple dimensions amongst tweets from their followers, we used principal component analysis to reduce the dimensionality of the data. The following is the distribution of our cleaned data.

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
# Check distribution for SM2
SM2 %>% 
  gather(Variable, Value) %>% 
  ggplot(aes(x=Value, fill=Variable)) +
  geom_density(alpha=0.3) +
  geom_vline(aes(xintercept=0)) +
  theme_bw()
```

We continue with principal component analysis.

```{r, include=FALSE}
SM_pca <- prcomp(SM2[,-c(1,10)],
                 scale. = TRUE,
                 center = TRUE)

str(SM_pca)
print(SM_pca)
```

```{r, echo=FALSE}
plot(SM_pca, type = 'l') 
```

```{r, include=FALSE}
SM_pcaDF <- data.frame(SM_pca$x)
```

The following is table about each principal component and a matrix of how they relate to each other.

```{r, echo=FALSE}
summary(SM_pca)
plot(SM_pcaDF[,1:5], pch = 16, col=rgb(0,0,0, alpha = .5), cex =.3)
```

```{r, include=FALSE}
# Plot of PCA
library(ggplot2)
library(ggfortify)
```

```{r, echo=FALSE}
ggplot2::autoplot(SM_pca, loadings = TRUE, label.size = 3, loadings.label = TRUE, loadings.colour = 'red', loadings.label.colour = "blue")
```

```{r, include=FALSE}
# Find loadings
loadings = SM_pca$rotation

# these are the 10 tweet groups most negatively associated with Nutrient H20 followers
loadings[,1] %>% sort %>% head(10)

# these are the 10 tweet groups most positively associated with Nutrient H20 followers
loadings[,1] %>% sort %>% tail(10)

#----Hierarchical Clustering the PCA's for market segmentation----#
x = loadings[,1]
y = loadings[,2:8]
# Cluster and graphing
```

We then used hierarchical clustering on these principal components to find correlational clusters amongst their followers tweets in order to segment the market into different groups.

```{r, echo=FALSE}
# Market segementation using the PCA's 1-8
hc = hclust(dist(cbind(x,y)), method = 'ward.D')
plot(hc, axes=F, xlab='Twitter Categories', ylab='Market Segmentation', sub ='', main='Clustering of Principle Components 1-8')
rect.hclust(hc, k=6, border='red')
```

In doing so we found 6 main marketing groups based on correlated interests. We believe that by focusing their attention on these six groups, NutrientH20 can direct their content in order to relate to these subgroups to further their sales. 

Problem 2
================
## Association rules for grocery purchases

